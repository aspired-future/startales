networks:
  startales-network:
    driver: bridge

services:
  # Frontend UI Service
  ui:
    build:
      context: ..
      dockerfile: docker_galaxy/Dockerfile.ui
    ports:
      - "5173:5173"
    volumes:
      - ../src:/app/src
      - ../vite.config.ts:/app/vite.config.ts
      - ../tsconfig.json:/app/tsconfig.json
    environment:
      - NODE_ENV=development
      - VITE_API_URL=http://api:4000
      - VITE_STT_URL=http://stt:8000
      - VITE_TTS_URL=http://tts:8000
    depends_on:
      - api
    restart: unless-stopped
    networks:
      - startales-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5173"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Main API Service
  api:
    build:
      context: ..
      dockerfile: docker_galaxy/services/api/Dockerfile
    ports:
      - "4000:4000"
    environment:
      - NODE_ENV=development
      - OLLAMA_BASE_URL=http://ollama:11434
      - DATABASE_URL=postgres://gtw:gtw@postgres:5432/gtw
      - STT_SERVICE_URL=http://stt:8000
      - TTS_SERVICE_URL=http://tts:8000
      - REDIS_URL=redis://redis:6379
      - PORT=4000
    volumes:
      - ../src:/app/src:ro  # Mount source code for hot reloading (read-only for security)
      - ../package.json:/app/package.json:ro
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_started
      nats:
        condition: service_started
      ollama:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - startales-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # STT (Speech-to-Text) Service
  stt:
    build:
      context: ..
      dockerfile: docker_galaxy/services/stt/Dockerfile
    ports:
      - "8001:8000"
    environment:
      - MODEL_SIZE=base
      - DEVICE=cpu
      - LANGUAGE=en
      - PORT=8000
    volumes:
      - stt_models:/app/models
    restart: unless-stopped
    networks:
      - startales-network
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # TTS (Text-to-Speech) Service
  tts:
    build:
      context: ..
      dockerfile: docker_galaxy/services/tts/Dockerfile
    ports:
      - "8002:8000"
    environment:
      - MODEL_NAME=tts_models/en/ljspeech/tacotron2-DDC
      - DEVICE=cpu
      - PORT=8000
    volumes:
      - tts_models:/app/models
      - tts_cache:/app/cache
    restart: unless-stopped
    networks:
      - startales-network
    deploy:
      resources:
        limits:
          memory: 3G
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # Ollama AI Service (LLMs + Embeddings)
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      - OLLAMA_NUM_PARALLEL=4          # Support concurrent LLM + embedding requests
      - OLLAMA_MAX_LOADED_MODELS=3     # Keep both LLM and embedding models loaded
      - OLLAMA_KEEP_ALIVE=5m           # Keep models in memory longer
    restart: unless-stopped
    networks:
      - startales-network
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G                   # Increased for multiple models
        reservations:
          cpus: '2.0'
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Ollama Model Initialization
  ollama-init:
    build:
      context: ..
      dockerfile: docker_galaxy/services/ollama/Dockerfile.init
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=ollama:11434
    networks:
      - startales-network
    restart: "no"

  # Redis for caching and sessions
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 1gb --maxmemory-policy allkeys-lru
    restart: unless-stopped
    networks:
      - startales-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Vector Database for AI embeddings
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped
    networks:
      - startales-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Message Queue for microservices
  nats:
    image: nats:2
    ports:
      - "4222:4222"
      - "8222:8222"
    restart: unless-stopped
    networks:
      - startales-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8222/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

  # PostgreSQL Database
  postgres:
    image: postgres:16
    environment:
      - POSTGRES_USER=gtw
      - POSTGRES_PASSWORD=gtw
      - POSTGRES_DB=gtw
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    restart: unless-stopped
    networks:
      - startales-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U gtw"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

volumes:
  ollama_models:
    driver: local
  stt_models:
    driver: local
  tts_models:
    driver: local
  tts_cache:
    driver: local
  redis_data:
    driver: local
  qdrant_data:
    driver: local
  pgdata:
    driver: local


